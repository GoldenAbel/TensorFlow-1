import tensorflow as tf
import numpy as np 
#定位符号变量，占位符
x = tf.placeholder(dtype=tf.float32, shape=None)
y = tf.placeholder(dtype=tf.float32, shape=None)
#构造乘法节点op
#y = tf.matmul(a, b)
#z = x*y
z = tf.mod(x, y, name=None)
print("设置种子的两种方式和区别")
tf.set_random_seed(1234)
#a = tf.random_uniform([1])
b = tf.random_normal([1])
a = tf.random_uniform([1], seed=1)


print("Session 1")
with tf.Session() as sess1:
    print(sess1.run(a))  # generates 'A1'
    print(sess1.run(a))  # generates 'A2'
    print(sess1.run(b))  # generates 'B1'
    print(sess1.run(b))  # generates 'B2'

print("Session 2")
with tf.Session() as sess2:
    print(sess2.run(a))  # generates 'A3'
    print(sess2.run(a))  # generates 'A4'
    print(sess2.run(b))  # generates 'B3'
    print(sess2.run(b))  # generates 'B4'

with tf.Session() as sess:
    value = sess.run(z, feed_dict={x: 3, y: 3})
    print(value)
    data = tf.zeros(shape = [2, 3], dtype = tf.int32, name = "tf.zeros")
    print(sess.run(data))
    
    tf_zeros_like_1 = tf.zeros_like(data)
    tf_zeros_like_2 = tf.zeros_like(data, tf.float32,name = "tf.zeros_like")
    print("这个函数返回一个全是零的张量，数据维度是和data一样，数据类型是默认是和data一样，但是我们也可以自己指定。")
    print(sess.run(tf_zeros_like_1))
    print(sess.run(tf_zeros_like_2))
    
    tf_ones = tf.ones(shape = [2, 3], dtype = tf.int32, name = "tf_ones")
    print("这个函数返回一个全是1的张量，数据维度是shape，数据类型是dtype")
    print(sess.run(data))
    
    tf_ones_like_1 = tf.ones_like(data,name = "tf.ones_like")
    tf_ones_like_2 = tf.ones_like(data, tf.float32,name = "tf.ones_like")
    print("这个函数返回一个全是'1'的张量，数据维度是和data一样，数据类型是默认是和data一样，但是我们也可以自己指定。")
    print(sess.run(tf_ones_like_1))
    print(sess.run(tf_ones_like_2))
    
    tf_fill = tf.fill(dims = [2,3],value = 9.,name = "tf.fill")
    print("这个函数返回一个Tensor，数据维度是dims，填充的数据都是value")
    print(sess.run(tf_fill))
    
    tf_constant1 = tf.constant([1, 2, 3],name = "tf.constant")
    print("这个函数返回一个常量Tensor,可以指定值和维度")
    print(sess.run(tf_constant1))
    tf_constant2 = tf.constant(-1.0, shape = [2, 3],name = "tf.constant")
    print(sess.run(tf_constant2))
    tf_constant3 = tf.constant(2.0, dtype = tf.float32, shape = [3, 2],name = "tf.constant")
    print(sess.run(tf_constant3))
    
    linspace = tf.linspace(start=10.0, stop=15.0, num=10,name = "tf.linspace")
    print("这个函数返回一个序列数组，数组的第一个元素是start，如果num>1，那么序列的最后一个元素就是 stop - start / num - 1。也就是说，最后一个元素肯定是stop")
    print("start到stop之间取num个数字,num个数中包括start和stop")
    print(sess.run(linspace))
    
    tf_range = tf.range(start=3, limit=15, delta=3,name="tf.range")
    print("这个函数返回一个序列数组，数组的第一个元素是start，之后的每一个元素都在前一个元素的基础上，加上delta，直到limit，但是不包括limit")
    print(sess.run(tf_range))

    tf_random_normal =tf.random_normal(shape=[2,3], mean = 10.0, stddev = 1.0, dtype = tf.float32, seed = None, name = "tf.random_normal")
    print("这个函数返回一个随机数序列，数组里面的值按照正态分布,mean为均值，stddev为方差")
    print(sess.run(tf_random_normal))
    
    tf_truncated_normal =tf.truncated_normal(shape=[2,3], mean = 10.0, stddev = 1.0, dtype = tf.float32, seed = None, name = "tf.truncated_normal")
    print("这个函数返回一个随机数序列，数组里面的值按照正态分布，但和random_normal函数不同的是，该值返回的是一个截断的正态分布类型。也就是说，产生出来的值范围都是在 [mean - 2 * standard_deviations, mean + 2 * standard_deviations]内,mean为均值，stddev为方差")
    print(sess.run(tf_truncated_normal))
    
    tf_random_uniform = tf.random_uniform(shape=[2,3], minval = 10.0, maxval = 20.0, dtype = tf.float32, seed = None, name = None)
    print("这个函数返回一个随机数序列，数组里面的值按照均匀分布，数据范围是 [minval, maxval)")
    print(sess.run(tf_random_uniform))
    
    test_data = tf.constant([[1, 2], [3, 4], [5, 6]])
    shuff_data1 = tf.random_shuffle(test_data,name = "tf_random_shuffle")
    print("这个函数返回一个随机数序列，将value中的数据打乱输出")
    print(sess.run(test_data))
    print(sess.run(shuff_data1))

    test_data = tf.constant([1, 2, 3, 4, 5, 6])
    shuff_data2 = tf.random_shuffle(test_data,name = "tf_random_shuffle")
    print("这个函数返回一个随机数序列，将value中的数据打乱输出")
    print(sess.run(test_data))
    print(sess.run(shuff_data2))

    tf_string_to_number = tf.string_to_number("100.01", out_type=None, name="tf_string_to_number")
    print("字符串转为数字")
    print(sess.run(tf_string_to_number))
    
    tf_to_double = tf.to_double(99.0, name="tf_to_double")
    print("转为double")
    print(sess.run(tf_to_double))
    
    #tensor a is [1.8, 2.2], dtype=tf.float
    tensor = [1.8, 2.2]
    tf_cast = tf.cast(x = tensor, dtype = tf.int32,name = "tf.cast") # dtype=tf.int32
    print("将x或者x.values转换为dtype")
    print(tensor)
    print(sess.run(tf_cast))
    
    tensor = [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
    tf_shape = tf.shape(input = tensor, name="tf.shape")
    print("返回数据的shape")
    print(tensor)
    print(sess.run(tf_shape))
    
    tf_size = tf.size(tensor, name="tf.size")
    print("返回数据的元素数量")
    print(tensor)
    print(sess.run(tf_size))
    
    tf_rank = tf.rank(tensor, name="tf.rank")
    print("返回tensor的矩阵的秩")
    print(tensor)
    print(sess.run(tf_rank))
    
    tensor = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    print("改变tensor的形状")
    print(tensor)
    print(sess.run(tf.shape(tensor)))
    tf_reshape = tf.reshape(tensor, [3, 3])
    print(sess.run(tf.shape(tf_reshape)))
    
    tensor = [
        [[1, 1, 1], [2, 2, 2]], 
        [[3, 3, 3], [4, 4, 4]]
        ]
    tf_expand_dims1 = tf.expand_dims(tensor, dim = 1, name="tf.expand_dims")
    tf_expand_dims0 = tf.expand_dims(tensor, dim = 0, name="tf.expand_dims")
    tf_expand_dims_1 = tf.expand_dims(tensor, dim = -1, name="tf.expand_dims")
    print("在dim的位置插入维度1进入一个tensor中")
    print(sess.run(tf.shape(tensor)))
    print(sess.run(tf.shape(tf_expand_dims1)))
    print(sess.run(tf_expand_dims1))
    print(sess.run(tf.shape(tf_expand_dims0)))
    print(sess.run(tf_expand_dims0))
    print(sess.run(tf.shape(tf_expand_dims_1)))
    print(sess.run(tf_expand_dims_1))
    
    tensor = [[[1, 1, 1], [2, 2, 2]],
              [[3, 3, 3], [4, 4, 4]],
              [[5, 5, 5], [6, 6, 6]]]
    tf_slice0 = tf.slice(tensor, begin=[1, 0, 0], size=[1, 1, 3],name = "tf.slice")
    tf_slice1 = tf.slice(tensor, begin=[1, 0, 0], size=[1, 2, 3],name = "tf.slice")
    tf_slice2 = tf.slice(tensor, begin=[1, 0, 0], size=[2, 1, 3],name = "tf.slice")
    print("对tensor进行切片操作")
    print("tf_slice0")
    print(sess.run(tf_slice0))
    print("tf_slice1")
    print(sess.run(tf_slice1))
    print("tf_slice2")
    print(sess.run(tf_slice2))
    
    '''tensor = [[1, 2, 3, 4, 5, 6, 7, 8, 9],
             [1, 2, 3, 4, 5, 6, 7, 8, 9]]
    split0, split1, split2 = tf.split(0,3,tensor, name="tf.split")
    print("split0")
    print(sess.run(split0))
    print("split1")
    print(sess.run(split1))
    print("split2")
    print(sess.run(split2))'''
    
    '''t1 = [[1, 2, 3], [4, 5, 6]]
    t2 = [[7, 8, 9], [10, 11, 12]]
    tf_concat0 = tf.concat(0, [t1, t2],name="tf.concat")
    tf_concat1 = tf.concat(1, [t1, t2],name="tf.concat")
    print("沿着某一维度连结tensor")
    print(sess.run(tf_concat0))
    print(sess.run(tf_concat1))'''
    
    aa=tf.constant([1,2,3],dtype=tf.float32)
    bb=tf.constant([[1,1],[2,2],[3,3]],dtype=tf.float32)
    print("output = sum(t ** 2) / 2")
    print("对权重参数进行正则化")
    print("a:")
    print(sess.run(tf.nn.l2_loss(aa)))
    print("b:")
    print(sess.run(tf.nn.l2_loss(bb)))
    
    '''
                注意：如果labels的每一行是one-hot表示，也就是只有一个地方为1，其他地方为0，可以使用tf.sparse_softmax_cross_entropy_with_logits()
    '''
    #每一行labels[i]必须为一个概率分布,和为1
    #[batch_size, num_classes]
    labels = [[0.2,0.3,0.5],
          [0.1,0.6,0.3]]
    labels1 = [0,2]
    #未缩放的对数概率
    logits = [[2,0.5,1],
          [0.1,1,3]]
    logits_scaled = tf.nn.softmax(logits)
    #长度为batch_size的一维Tensor
    result1 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)
    result2 = -tf.reduce_sum(labels*tf.log(logits_scaled),1)
    result3 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits_scaled)
    result4 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels1, logits=logits)
    print("交叉熵")
    print(sess.run(result1))
    print(sess.run(result2))
    print(sess.run(result3))
    print(sess.run(result4))
    
    #判断target 是否在top k 的预测之中
    tensor = [1, 2, 3, 4, 5, 6, 7, 8, 9,10]
    A = [[0.8,0.6,1.0], [0.1,0.6,0.4]]  
    B = [2, 1]  
    #init=tf.global_variables_initializer()
    #sess.run(init)
    tf_nn_top_k = tf.nn.top_k(input = tensor, k=2, sorted=True, name="tf.nn.top_k")
    tf_nn_in_top_k = tf.nn.in_top_k(predictions=A, targets=B, k=1, name="tf.nn.in_top_k")
    print("返回前k大的值及其对应的索引")
    print(sess.run(tf_nn_top_k))
    print("返回前k大的值及其对应的索引,targets中索引是不是对应A中的最大值的索引，是就是True")
    print(sess.run(tf_nn_in_top_k))
    
    W = tf.constant([[-2.,12.,6.],[3.,2.,8.]])
    mean,var = tf.nn.moments(W, axes = [0])
    print("直接计算均值与方差,按列来计算：")
    resultMean = sess.run(mean)
    print(resultMean)
    resultVar = sess.run(var)
    print(resultVar)
    
    batch_size = 5
    ones = tf.ones([batch_size,20])
    logits = tf.layers.dense(inputs=ones,units=10,name="tf.layers.dense")
    print("tf.layers.dense实例,inputs: 输入数据，2维tensor,units: 该层的神经单元结点数.")
    print(ones.get_shape())
    print(logits.get_shape())
    
    
    batch_size = 5
    ones = tf.ones([batch_size,8,20])
    logits = tf.layers.dense(inputs=ones,units=10)
    print("tf.layers.dense实例,inputs: 输入数据，2维tensor,units: 该层的神经单元结点数.")
    print(ones.get_shape())
    print(logits.get_shape())
    
    batch_size = 5
    ones = tf.ones([batch_size,6,8,20])
    logits = tf.layers.dense(ones,10)
    print("tf.layers.dense实例,inputs: 输入数据，2维tensor,units: 该层的神经单元结点数.")
    print(ones.get_shape())
    print(logits.get_shape())
    
    v1 = tf.get_variable(name='v1', shape=[1], initializer=tf.constant_initializer(10))  
    tf.add_to_collection('loss', v1)  
    v2 = tf.get_variable(name='v2', shape=[1], initializer=tf.constant_initializer(2))  
    tf.add_to_collection('loss', v2)  
  
    sess.run(tf.global_variables_initializer())  
    print("tf.add_to_collection：把变量放入一个集合，把很多变量变成一个列表 \n"+
    "tf.get_collection：从一个结合中取出全部变量，是一个列表\n"+
    "tf.add_n：把一个列表的东西都依次加起来\n")
    print(tf.get_collection('loss'))
    print(sess.run(tf.add_n(tf.get_collection('loss'))))  
    #执行会话，输入数据，计算节点，打印结果
    #任务完成，关闭会话
